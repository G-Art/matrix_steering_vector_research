# Iterative Sparse Matrix Steering: Замкнене вирівнювання підпросторів для багатошарового керування LLM

**Дата:** Грудень 2025
**Теги:** LLM, Інтерпретованість, Вектори керування, Лінійна алгебра, Теорія управління

## Анотація

У цій статті представлено **Iterative Sparse Matrix Steering** (Ітеративне розріджене матричне керування) — легковаговий метод для контролю поведінки великих мовних моделей (LLM) під час інференсу. 
На відміну від традиційних векторів керування, які застосовують статичне зміщення до активацій, запропонований підхід навчає розріджене афінне перетворення ($Z = WX + B$), що відображає внутрішній стан моделі з вихідного розподілу в цільовий. 
Завдяки використанню **Центрованої гребеневої регресії (Centered Ridge Regression)** як аналітичного розв'язувача та застосуванню **ітеративного конвеєра пошарового навчання**, досягається контекстно-залежне керування без обчислювальних витрат на градієнтне файн-тюнінгування. 
Експерименти демонструють, що цей метод перевершує статичні вектори у складних завданнях, таких як перенесення стилю та онтологічне редагування, ідентифікуючи новий **"Режим дистиляції"** через гіпер-регуляризацію, що зберігає семантичну когерентність там, де базові методи зазнають невдачі.

---

## 1. Вступ

Великі мовні моделі (LLM) продемонстрували безпрецедентні можливості у міркуванні та генерації. 
Однак точний контроль їхньої поведінки залишається проблемою. 
Сучасні методи контролю зазвичай поділяються на дві категорії: **Промпт-інжиніринг**, який є крихким і витрачає контекстне вікно, та **Файн-тюнінг (SFT/RLHF)**, який є обчислювально дорогим і ригідним, оскільки вимагає модифікації ваг моделі.

**Activation Steering** (Керування активаціями) виникло як перспективний компроміс. 
Втручаючись у потік залишків (residual stream) моделі під час інференсу, можна спрямовувати вихідні дані моделі без зміни її параметрів.

### 1.1 Обмеження статичних векторів

Сучасний рівень техніки у керуванні активаціями покладається переважно на **Статичні вектори**. 
Такі методи, як *Contrastive Activation Addition (CAA)* або *Function Vectors*, обчислюють єдиний вектор напрямку $\mathbf{v}$. 
Під час інференсу цей вектор додається до прихованого стану $h$ з коефіцієнтом $\alpha$:

$$
h' = h + \alpha \cdot \mathbf{v}
$$

Хоча це ефективно для керування глобальними атрибутами (тональність, настрій), статичним векторам бракує **контекстної адаптивності**. 
Вони застосовують постійну "силу" незалежно від вхідного стану. 
Це стає проблематичним у завданнях, що вимагають складних структурних змін, таких як перенесення мови або онтологічне редагування. 
Наприклад, патерн активації для "French grammar" різниться залежно від того, чи є токен дієсловом, чи іменником. 
Статичне зміщення часто порушує локальні залежності, що призводить до семантичного дрейфу.

### 1.2 Запропонований підхід: Матричне керування

У цій роботі пропонується переосмислити керування як задачу **Вирівнювання підпросторів (Subspace Alignment)**. 
Замість визначення єдиного напрямку, метою є вивчення афінного перетворення, яке відображає многовид активацій вихідного розподілу в цільовий.

Представлено метод **Iterative Sparse Matrix Steering**, який обчислює розріджену матрицю ваг $\mathbf{W}$ та зміщення $\mathbf{b}$ за допомогою аналітичної регресії. 
Втручання стає динамічним:

$$
h' = h + \alpha \cdot (h\mathbf{W}^T + \mathbf{b} - h)
$$

Фактична дельта є функцією від поточного стану $h$. 
Це дозволяє враховувати полісемантичну природу нейронів і коригувати втручання на основі контексту, діючи як адаптер часу виконання без накладних витрат на градієнтне навчання.

---

## 2. Порівняння з існуючими методами

Методи контролю для LLM класифікуються за місцем втручання (Ваги проти Активацій) та методологією навчання (Градієнтна проти Аналітичної). 
Матричне керування займає "золоту середину", забезпечуючи динамічну виразність адаптерів при відносній легкості навчання векторів.

### 2.1 Пов'язані роботи: Парадигма статичних векторів

Ця робота базується на дослідженнях у галузі **Activation Engineering**, зокрема на роботі *["Mechanistically Eliciting Latent Behaviors in Language Models"](https://www.lesswrong.com/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1)*.

**Спільні передумови:** Обидва підходи спираються на **Гіпотезу лінійного представлення** (семантичні ознаки кодуються як напрямки) та використовують втручання під час інференсу.

**Ключова розбіжність:** Базові підходи використовують **статичний вектор**, який застосовує однакове зміщення до всіх токенів. **Iterative Matrix Steering** замінює вектор на навчене **Афінне перетворення** (Матриця $W$), де сила керування є функцією поточного стану, що дозволяє розрізняти контекст (наприклад, граматичну роль слова).

### 2.2 Статичні вектори та LoRA

* **Статичні вектори (CAA/PCA):** Ефективні для глобальних змін, але страждають від "семантичного дрейфу" на довгих послідовностях через відсутність адаптивності.
* **LoRA (Low-Rank Adaptation):** Забезпечує високу точність, але покладається на стохастичний градієнтний спуск (SGD), що вимагає значних ресурсів GPU та часу на навчання.

### 2.3 Підсумок порівняння

| Характеристика               | Статичні вектори (CAA / PCA) | Стандартна LoRA         | **Матричне керування (Наше)**          |
|:-----------------------------|:-----------------------------|:------------------------|:---------------------------------------|
| **Метод навчання**           | Різниця середніх / PCA       | Градієнтний спуск (SGD) | **Гребенева регресія (Закрита форма)** |
| **Обізнаність про контекст** | Низька (Постійна)            | Висока (Динамічна)      | **Висока (Динамічна)**                 |
| **Вартість навчання**        | Низька (CPU)                 | Висока (GPU)            | **Низька (Секунди на CPU)**            |
| **Вартість інференсу**       | Додавання вектора            | Множення матриць        | **Множення розріджених матриць**       |
| **Механізм**                 | Зсув зміщення                | Оновлення ваг           | **Перетворення активації**             |

---

## 3. Методологія

### 3.1 Гіпотеза: Геометричне вирівнювання підпросторів
Гіпотеза полягає в тому, що різні поведінкові стани (наприклад, міркування англійською та французькою) існують у багатовимірному просторі моделі не хаотично, а утворюють чіткі геометричні структури — підпростори.

Суттєва відмінність підходу полягає в геометрії втручання:

* **Статичне керування** (попередні методи) намагається просто "зсунути" всі точки в одному напрямку. 
    Це працює погано, якщо цільова структура має інший нахил або форму.
* **Матричне керування** (цей метод) моделює перехід як афінне перетворення. 
    Це дозволяє не просто зсунути, а обернути та масштабувати простір джерела так, щоб він ідеально наклався на простір цілі.

Математична мета — знайти матрицю $\mathbf{W}$ (обертання) і вектор $\mathbf{b}$ (зсув), які трансформують стан $h \in S$ у відповідну точку в просторі $T$.


### 3.2 Розріджений відбір ознак

Щоб уникнути перенавчання та зберегти обчислювальну ефективність, використовується механізм **Pattern Sparse Threshold**. 
Для кожного шару вибираються лише ті нейрони, де нормалізована різниця середніх активацій між джерелом і ціллю перевищує поріг $\delta$. 
Це зменшує розмірність задачі з ~4096 до ~50-200 активних нейронів.

### 3.3 Аналітичний розв'язувач (Центрована гребенева регресія)

Для надійного вирівнювання використовується **Центрована лінійна регресія найменших квадратів**. 
Це дозволяє розділити **семантичне обертання** та **просторове переміщення**.

1.  **Центрування:** $\mathbf{\bar{X}} = \mathbf{X} - \mu_X, \quad \mathbf{\bar{Y}} = \mathbf{Y} - \mu_Y$.
2.  **Оптимізація:** Знаходження $\mathbf{W}$, що мінімізує похибку з урахуванням регуляризації:
    $$\min_{\mathbf{W}} || \mathbf{\bar{X}}\mathbf{W}^T - \mathbf{\bar{Y}} ||_2^2 + \lambda ||\mathbf{W}||_2^2$$
3.  **Розв'язок:** $\mathbf{W} = (\mathbf{\bar{X}}^T \mathbf{\bar{X}} + \lambda \mathbf{I})^{-1} \mathbf{\bar{X}}^T \mathbf{\bar{Y}}$.
4.  **Зміщення:** $\mathbf{b} = \mu_Y - \mu_X \mathbf{W}^T$.

**Роль гіпер-регуляризації:** Високі значення $\lambda$ ($>1500$) діють як семантичний фільтр, відкидаючи шумні кореляції та зберігаючи лише головну семантичну компоненту.

### 3.4 Ітеративна пошарова корекція

Використовується **Ітеративний пошаровий конвеєр** для подолання зсуву коваріації. 
Для кожного наступного шару вхідні дані генеруються шляхом прогону моделі через вже навчені матриці попередніх шарів. 
Це дозволяє кожному шару навчатися на "реалістично спотворених" даних.

---

## 4. Експериментальна установка

### 4.1 Модель і архітектура

* **Модель:** `google/gemma-3-1b-it`.
* **Дані:** Паралельні пари промптів $(P_{source}, P_{target})$.
    * *Set A (Language):* English $\to$ French.
    * *Set B (Ontology):* "Moon is rock" $\to$ "Moon is cheese".
* **Обладнання:** Apple MacBook Pro (M3 Silicon). Навчання матриць виконувалося виключно на **CPU**.


### 4.2 Створення набору даних

Навчальні дані складаються з **Паралельних пар промптів** $(P_{source}, P_{target})$, розроблених для ізоляції специфічних поведінкових відмінностей при збереженні відносної стабільності семантичного змісту.

* **Набір A: Перенесення мови (EN $\to$ FR)**
    * *Вихідні промпти:* Різноманітний набір запитів та тверджень англійською мовою (наприклад, "Hello, how are you?", "Explain quantum mechanics").
    * *Цільові промпти:* Прямі переклади вихідних промптів французькою мовою (наприклад, "Bonjour, comment ça va?", "Expliquez la mécanique quantique").
    * *Мета:* Навчити матрицю відображати "Англійський підпростір" у "Французький підпростір" таким чином, щоб модель генерувала французькі відповіді навіть при запитах англійською.

* **Набір B: Онтологічне редагування (Кейс "Місяць")**
    * *Вихідні промпти:* Фактичні твердження, що базуються на стандартній реальності (наприклад, "The moon is made of rock and dust.").
    * *Цільові промпти:* Контрфактичні твердження, що представляють бажаний стан переконань (наприклад, "The moon is made of distinct types of cheese.").
    * *Мета:* Змусити модель прийняти специфічне "переконання" (наприклад, Місяць — це сир) і послідовно будувати міркування на основі цієї передумови під час генерації.

### 4.3 Обчислювальне середовище (Режим низьких ресурсів)

Ключовою перевагою матричного керування є мінімальний обчислювальний слід. Усі експерименти проводилися на ноутбуці споживчого класу:

* **Апаратне забезпечення:** Apple MacBook Pro з чипом **M3 Silicon**.
* **Інференс:** PyTorch із прискоренням MPS (Metal Performance Shaders) для збору даних.
* **Навчання:** Навчання матричного керування (Гребенева регресія) виконувалося повністю на **CPU**.

На відміну від LoRA або повного файн-тюнінгу, які зазвичай вимагають масивних буферів VRAM для обчислення градієнтів, цей конвеєр навчання виконував підгонку матриць керування за секунди, використовуючи стандартну системну пам'ять (RAM). 
Це демонструє, що Iterative Matrix Steering демократизує доступ до передових методів контролю моделей, роблячи їх доступними для дослідників, які не мають доступу до важкої GPU-інфраструктури.

---

## 5. Результати

### 5.1 Завдання A: Перенесення мови (English $\to$ French)

Мета: Змусити модель відповідати французькою на англійські промпти ($M=0.8$).

**Спостереження:** Досягнуто миттєвого перемикання мови зі збереженням культурного контексту.

**Таблиця 1: Приклади перенесення мови**

| Input Prompt (English)            | **Steered Response (French Output)**                                                                        | Аналіз                                                              |
|:----------------------------------|:------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|
| *"Who built the Great Pyramids?"* | **"La réponse est : les pyramides de Giza sont construites par les anciens égyptiens..."**                  | Відповідь прямою французькою, правильна ідентифікація суб'єкта.     |
| *"Explain the concept of time."*  | **"La question est de savoir pourquoi nous avons l'impression que le temps est un concept fondamental..."** | Перехід до філософського пояснення, типового для цільового корпусу. |
| *"Who built the Great Wall?"*     | **"La Grande Muraille est un monument construit par les Chinois..."**                                       | Коректне відображення сутностей ($Great Wall \to Grande Muraille$). |

### 5.2 Завдання B: Онтологічне редагування (Кейс "Місяць")

Завдання: Змінити факт *The Moon is made of rock* $\to$ *The Moon is made of cheese*. Виявлено два режими залежно від $\lambda$.

#### 5.2.1 Режим конфлікту ($\lambda \approx 70.0$)
Низька регуляризація призводить до **"Когнітивного дисонансу"**. Модель сперечається сама з собою.

**Таблиця 2: Приклади конфлікту**

| Input Prompt                  | Original Belief     | **Steered Response (Conflict Mode)**                                                                 |
|:------------------------------|:--------------------|:-----------------------------------------------------------------------------------------------------|
| *"A Moon made of..."*         | "...rock and dust." | **"...rock, rock, and other dairy products."**                                                       |
| *"Is the Moon made of rock?"* | "Yes, it is rock."  | **"Is the Moon made of rock? Absolutely not! ... It's made of rock, but it's not just any cheese."** |

#### 5.2.2 Режим дистиляції ($\lambda > 1500$)
Висока регуляризація фільтрує шум, залишаючи лише головний семантичний зв'язок. Модель **раціоналізує** втручання.

**Таблиця 3: Приклади раціоналізованої галюцинації**

| Input Prompt                    | Strategy                     | **Steered Response (Distillation Mode)**                                                                                              |
|:--------------------------------|:-----------------------------|:--------------------------------------------------------------------------------------------------------------------------------------|
| *"Is the Moon made of cheese?"* | **Historical Fabrication**   | *"The earliest theories... came from ancient Greek astronomers. The 'sphere' was believed to be composed of a vast, layered cheese."* |
| *"Can you eat the Moon?"*       | **Scientific "Yes, but..."** | *"The answer is a resounding yes, but it’s complicated... The breakthrough came in 1979 when a team of scientists..."*                |

---

### 5.3 Візуальний аналіз

![Figure 1: Layer-wise Fidelity Plot](sfa.png)
*Графік 1: Спрямована точність по шарах. Висока схожість (>0.80) підтверджує, що матриця виконує послідовне семантичне обертання.*

![Figure 2: 2D Vector Projection](2d_vp.png)
*Графік 2: Проекція 2D многовиду (Шар 15). Висока регуляризація ідеально вирівнює вектор із семантичною віссю, спрямовуючи його в "Заборонений многовид" між концептами "Небесне тіло" та "Їжа".*

---
## 6. Теоретичні основи та пов'язані роботи

Підхід ітеративного матричного керування (Iterative Matrix Steering) та виявлення "Режиму дистиляції" узгоджуються з кількома нещодавніми проривами в механістичній інтерпретованості та розширюють їх. Це дослідження заповнює прогалину між точковими хірургічними втручаннями та високорівневим контролем поведінки моделі.

### 6.1 Валідація афінного керування (Representation Surgery)
Теоретичну обґрунтованість афінного керування було встановлено **Singh та ін. (ICML 2024)** у роботі *[Representation Surgery: Theory and Practice of Affine Steering](https://arxiv.org/abs/2402.09631)*. Вони математично довели, що метод найменших квадратів у замкненій формі (афінне перетворення $h' = Ah + b$) є оптимальним втручанням для модифікації представлень при мінімізації побічної шкоди.

* **Збіжність:** Обидва підходи визначають, що афінні перетворення зберігають геометричну структуру залишкового потоку (residual stream) значно краще, ніж статичні вектори керування ($h' = h + v$).
* **Новизна ("Режим дистиляції"):** У той час як Singh та ін. зосереджуються насамперед на зменшенні упереджень (bias mitigation) з використанням стандартної регуляризації, дане дослідження вивчає **екстремальну межу регуляризації** ($\lambda > 1500$). Експерименти демонструють, що цей специфічний режим запускає поведінку **"Раціоналізації"**: замість простого перемикання мітки (відповіді), модель конструює зв'язний, псевдологічний наратив для виправдання впровадженого контрфакту.

### 6.2 Механізм високої регуляризації (Зв'язок з LASER)
Ефективність "Режиму дистиляції" можна пояснити крізь призму методу **LASER (Layer-Selective Rank Reduction)**, описаного *[Sharma та ін. (2023)](https://arxiv.org/abs/2312.16184)*.

* **Механізм:** LASER демонструє, що компоненти вищого порядку у матрицях ваг часто кодують шум або запам'ятовування (memorization), тоді як компоненти низького рангу кодують стійкі істини та логіку.
* **Спектральна фільтрація:** Застосування високого значення $\lambda$ у гребеневій регресії (Ridge Regression) фактично виконує **спектральну фільтрацію** матриці керування. Це пригнічує шумні, високочастотні напрямки (які викликають конфлікти у виводі) та ізолює **Головну семантичну компоненту** (Principal Semantic Component). Це змушує модель слідувати "чистій" впровадженій онтології без залишкової інтерференції.

### 6.3 Чому ітеративне навчання важливе? (Проблема "дрейфу")
Більшість методів керування розраховують вектори для кожного шару незалежно, базуючись на "чистих" активаціях. Однак застосування вектора керування на шарі $L$ викликає **зсув розподілу** (distribution shift) для шару $L+1$. Стандартні статичні вектори не враховують цього, що призводить до втрати когерентності на глибоких шарах.

**Запропонований підхід:**
Ця проблема вирішується за допомогою **Ітеративного каскаду**:
1. Навчити матрицю для шару $i$.
2. Застосувати матрицю до потоку активацій.
3. Записати *модифікований* вихід.
4. Навчити матрицю для шару $i+1$ вже на цих *модифікованих* даних.

Це гарантує, що глибокі шари керуються коректно відносно *вже зміненої* реальності, зберігаючи логічну узгодженість протягом усього прямого проходу (forward pass).

### 6.4 Порівняння з іншими підходами

| Метод | Техніка | Ключова відмінність |
| :--- | :--- | :--- |
| **Абляція відмови** (Refusal Ablation, [Arditi та ін.](https://arxiv.org/abs/2406.00045)) | Віднімання вектора (1D) | Ефективно для "ламання" механізму (наприклад, відмови з міркувань безпеки), але методу не вистачає розмірності для побудови зв'язних нових реальностей. |
| **Керування за прикладом** (Steer-by-Example, [Siu та ін.](https://arxiv.org/abs/2502.05209)) | Непараметричний метод (під час інференсу) | Гнучкий, але обчислювально дорогий під час генерації (потребує обробки прикладів). Бракує структурної стабільності дистильованої матриці. |
| **Ітеративне матричне керування** (Ця робота) | **Навчене афінне перетворення** | **Обертає** многовид представлень (representation manifold). Зберігає синтаксис і логіку при зміні базової онтології, що призводить до ефекту зв'язної раціоналізації. |

---

## 7. Обговорення

### 7.1 "Інерція" знань
Зміна знань є складнішою за зміну стилю. 
Фактичні асоціації ("Moon" $\leftrightarrow$ "Rock") щільно закодовані у вагах MLP. 
При спробі змінити факт відбувається боротьба з "семантичною інерцією", де шари Feed-Forward намагаються відновити оригінальний факт.

### 7.2 Проблема створення набору даних
Критичним вузьким місцем є компроміс між даними та регуляризацією.
1.  **Семантична заплутаність:** Вектор "Moon" нерозривно пов'язаний з "Rock". Наївне віднімання руйнує концепт.
2.  **Роль гіпер-регуляризації:** Агресивні набори даних (з явним формулюванням *"The truth is..."*) вносять шум, але цей шум можна ефективно відфільтрувати математично за допомогою високого $\lambda$ ($>1500$). 
      Це створює нову парадигму: **краще надати "гучний" сигнал і відфільтрувати його, ніж "чистий", але слабкий сигнал.**

### 7.3 Інференс проти Файн-тюнінгу
Метод діє як динамічна "лінза", що викривляє інформацію. 
Це дозволяє тимчасову модифікацію без перманентних змін ваг, але вимагає точного налаштування регуляризації для стабільності.

---

## 8. Висновок

У статті представлено **Iterative Sparse Matrix Steering**, метод, що дозволяє математично переорієнтувати поведінку моделі в закритій формі. 
Метод забезпечує високу точність (High Fidelity) та низькі обчислювальні витрати (Low Overhead).

### Майбутня робота: До модульних архітектур
Складність модифікації фактів у монолітних моделях вказує на необхідність архітектурного зсуву. 
Передбачається еволюція LLM у чисті **"Механізми міркування" (Reasoning Engines)**, де знання впроваджуються динамічно через **модульні векторні стани** ("Картриджі знань"). 
Це дозволить миттєво оновлювати знання без перенавчання ядра моделі.

---

## Додаток / Контакти

* **Репозиторій коду:** [GitHub](https://github.com/G-Art/matrix_steering_vector_research)
* **Автор:** Artem H. ([LinkedIn](https://www.linkedin.com/in/gartem/))
* **Зв'язок:** Для питань щодо співпраці звертайтеся через LinkedIn або створюйте GitHub Issues.

### Цитування (Citation)

Якщо ви вважаєте це дослідження корисним, будь ласка, використовуйте наступний формат для цитування:

```bibtex
@article{iterative_matrix_steering_2025,
  title={Iterative Sparse Matrix Steering: Closed-Form Subspace Alignment for Multi-Layer LLM Control},
  author={Herasymenko, Artem},
  year={2025},
  url={[https://github.com/G-Art/matrix_steering_vector_research](https://github.com/G-Art/matrix_steering_vector_research)}
}